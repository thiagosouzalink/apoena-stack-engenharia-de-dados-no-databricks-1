{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "681c0f7b-7fd5-42e8-a4a6-213485ec94ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>5</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 1
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO client\n",
    "VALUES \n",
    "(6, \"Iury\", \"iury@example.com\"),\n",
    "(7, \"Davi\", \"davi@email.com\"),\n",
    "(8, \"Fernando\", \"fernadinho@hotmail.com\"),\n",
    "(9, \"Pablo\", \"blo@example.org\"),\n",
    "(10, \"Helena\", \"btw@example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3c635f-0015-4091-a856-8d0ddf30472e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>5</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 2
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "INSERT INTO client\n",
    "VALUES \n",
    "(11, \"Pedro\", \"pedr@example.com\"),\n",
    "(12, \"Livia\", \"li@email.com\"),\n",
    "(13, \"Luciana\", \"lu@hotmail.com\"),\n",
    "(14, \"Vinicius\", \"vini@example.org\"),\n",
    "(15, \"Sirio\", \"siriu@example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "776f6df6-db98-42dd-bc22-b71c30c4f0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th></tr></thead><tbody><tr><td>delta</td><td>67ceea2f-ce76-4487-9423-553dd919fa2f</td><td>spark_catalog.default.client</td><td>null</td><td>dbfs:/user/hive/warehouse/client</td><td>2025-06-14T13:30:03.531Z</td><td>2025-06-14T14:37:28Z</td><td>List()</td><td>List()</td><td>3</td><td>3783</td><td>Map()</td><td>1</td><td>2</td><td>List(appendOnly, invariants)</td><td>Map()</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "delta",
         "67ceea2f-ce76-4487-9423-553dd919fa2f",
         "spark_catalog.default.client",
         null,
         "dbfs:/user/hive/warehouse/client",
         "2025-06-14T13:30:03.531Z",
         "2025-06-14T14:37:28Z",
         [],
         [],
         3,
         3783,
         {},
         1,
         2,
         [
          "appendOnly",
          "invariants"
         ],
         {}
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 3
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createdAt",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "lastModified",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "partitionColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "clusteringColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "numFiles",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sizeInBytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "minReaderVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "minWriterVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tableFeatures",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "statistics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"long\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e11e538-1559-44b0-ae6f-9760f00c8431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>metrics</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client</td><td>List(1, 3, List(1551, 1551, 1551.0, 1, 1551), List(1260, 1263, 1261.0, 3, 3783), 0, List(minCubeSize(107374182400), List(0, 0), List(3, 3783), 0, List(3, 3783), 1, null), null, 0, 1, 3, 0, false, 0, 0, 1749912025783, 1749912034537, 8, 1, null, List(0, 0), 3, 3, 299, 0, null)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client",
         [
          1,
          3,
          [
           1551,
           1551,
           1551.0,
           1,
           1551
          ],
          [
           1260,
           1263,
           1261.0,
           3,
           3783
          ],
          0,
          [
           "minCubeSize(107374182400)",
           [
            0,
            0
           ],
           [
            3,
            3783
           ],
           0,
           [
            3,
            3783
           ],
           1,
           null
          ],
          null,
          0,
          1,
          3,
          0,
          false,
          0,
          0,
          1749912025783,
          1749912034537,
          8,
          1,
          null,
          [
           0,
           0
          ],
          3,
          3,
          299,
          0,
          null
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "metrics",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"numFilesAdded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"filesAdded\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"filesRemoved\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"min\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"max\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"avg\",\"type\":\"double\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"partitionsOptimized\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"zOrderStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"strategyName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"inputCubeFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputOtherFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputNumCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"num\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numOutputCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedNumCubes\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"clusteringStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"inputZCubeFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"numFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputOtherFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"numFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"inputNumZCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"mergedFiles\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"numFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"size\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numOutputZCubes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numBins\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numBatches\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalConsideredFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalFilesSkipped\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"preserveInsertionOrder\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesSkippedToReduceWriteAmplification\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numBytesSkippedToReduceWriteAmplification\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"startTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"endTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalClusterParallelism\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalScheduledTasks\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"autoCompactParallelismStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"maxClusterActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minClusterActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"maxSessionActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minSessionActiveParallelism\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"deletionVectorStats\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"numDeletionVectorsRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numDeletionVectorRowsRemoved\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"numTableColumns\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numTableColumnsWithStats\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalTaskExecutionTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"skippedArchivedFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"clusteringMetrics\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"sizeOfTableInBytesBeforeLazyClustering\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"isNewMetadataCreated\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}},{\"name\":\"isPOTriggered\",\"type\":\"boolean\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesSkippedWithoutStats\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesClassifiedToIntermediateNodes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"sizeOfFilesClassifiedToIntermediateNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"logicalSizeOfFilesClassifiedToIntermediateNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesClassifiedToLeafNodes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"sizeOfFilesClassifiedToLeafNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"logicalSizeOfFilesClassifiedToLeafNodesInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numThreadsForClassifier\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"clusterThresholdStrategy\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"minFileSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"maxFileSize\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"nodeMinNumFilesToCompact\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numIdealFiles\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numClusteringTasksPlanned\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numCompactionTasksPlanned\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numOptimizeBatchesPlanned\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numLeafNodesExpanded\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numLeafNodesClustered\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numGetFilesForNodeCalls\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numSamplingJobs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numLeafNodesCompacted\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numIntermediateNodesCompacted\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSizeOfDataToCompactInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalLogicalSizeOfDataToCompactInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numIntermediateNodesClustered\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"numFilesSkippedAfterExpansion\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSizeOfFilesSkippedAfterExpansionInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalLogicalSizeOfFilesSkippedAfterExpansionInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSizeOfDataToRewriteInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalLogicalSizeOfDataToRewriteInBytes\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"timeMetrics\",\"type\":{\"type\":\"struct\",\"fields\":[{\"name\":\"classifierTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"optimizerTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"metadataLoadTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalGetFilesForNodeCallsTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"totalSamplingTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"metadataCreationTimeMs\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}},{\"name\":\"maxOptimizeBatchesInParallel\",\"type\":\"long\",\"nullable\":false,\"metadata\":{}},{\"name\":\"currentIteration\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"maxIterations\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}},{\"name\":\"clusteringStrategy\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]},\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "OPTIMIZE client\n",
    "ZORDER BY (id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331ea312-04af-4d8c-a653-8d6acea2b89f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>format</th><th>id</th><th>name</th><th>description</th><th>location</th><th>createdAt</th><th>lastModified</th><th>partitionColumns</th><th>clusteringColumns</th><th>numFiles</th><th>sizeInBytes</th><th>properties</th><th>minReaderVersion</th><th>minWriterVersion</th><th>tableFeatures</th><th>statistics</th></tr></thead><tbody><tr><td>delta</td><td>67ceea2f-ce76-4487-9423-553dd919fa2f</td><td>spark_catalog.default.client</td><td>null</td><td>dbfs:/user/hive/warehouse/client</td><td>2025-06-14T13:30:03.531Z</td><td>2025-06-14T14:40:31Z</td><td>List()</td><td>List()</td><td>1</td><td>1551</td><td>Map()</td><td>1</td><td>2</td><td>List(appendOnly, invariants)</td><td>Map()</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "delta",
         "67ceea2f-ce76-4487-9423-553dd919fa2f",
         "spark_catalog.default.client",
         null,
         "dbfs:/user/hive/warehouse/client",
         "2025-06-14T13:30:03.531Z",
         "2025-06-14T14:40:31Z",
         [],
         [],
         1,
         1551,
         {},
         1,
         2,
         [
          "appendOnly",
          "invariants"
         ],
         {}
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 5
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "location",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "createdAt",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "lastModified",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "partitionColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "clusteringColumns",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "numFiles",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "sizeInBytes",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "properties",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "minReaderVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "minWriterVersion",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "tableFeatures",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "statistics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"long\",\"valueContainsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ea5cb1-b4b8-4c5c-a0b4-bf4b1547521d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>7</td><td>2025-06-14T14:40:31Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>OPTIMIZE</td><td>Map(predicate -> [], auto -> false, clusterBy -> [], zOrderBy -> [\"id\"], batchId -> 0)</td><td>null</td><td>List(3289096376091294)</td><td>0614-132416-8nqtb2y</td><td>6</td><td>SnapshotIsolation</td><td>false</td><td>Map(numRemovedFiles -> 3, numRemovedBytes -> 3783, p25FileSize -> 1551, numDeletionVectorsRemoved -> 0, minFileSize -> 1551, numAddedFiles -> 1, maxFileSize -> 1551, p75FileSize -> 1551, p50FileSize -> 1551, numAddedBytes -> 1551)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>6</td><td>2025-06-14T14:37:28Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3289096376091294)</td><td>0614-132416-8nqtb2y</td><td>5</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1260)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>5</td><td>2025-06-14T14:37:17Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3289096376091294)</td><td>0614-132416-8nqtb2y</td><td>4</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1263)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>4</td><td>2025-06-14T14:03:38Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>RESTORE</td><td>Map(version -> 2, timestamp -> null)</td><td>null</td><td>List(3289096376091265)</td><td>0614-132416-8nqtb2y</td><td>3</td><td>Serializable</td><td>false</td><td>Map(numRestoredFiles -> 1, removedFilesSize -> 0, numRemovedFiles -> 0, restoredFilesSize -> 1260, numOfFilesAfterRestore -> 1, tableSizeAfterRestore -> 1260)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>3</td><td>2025-06-14T14:02:24Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>DELETE</td><td>Map(predicate -> [\"true\"])</td><td>null</td><td>List(3289096376091265)</td><td>0614-132416-8nqtb2y</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numRemovedBytes -> 1260, numCopiedRows -> 0, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 66, numDeletionVectorsUpdated -> 0, numDeletedRows -> 5, scanTimeMs -> 65, numAddedFiles -> 0, numAddedBytes -> 0, rewriteTimeMs -> 0)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>2</td><td>2025-06-14T13:37:06Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>UPDATE</td><td>Map(predicate -> [\"(nome#1479 = Carlos)\"])</td><td>null</td><td>List(3289096376091242)</td><td>0614-132416-8nqtb2y</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numRemovedFiles -> 1, numRemovedBytes -> 1257, numCopiedRows -> 4, numDeletionVectorsAdded -> 0, numDeletionVectorsRemoved -> 0, numAddedChangeFiles -> 0, executionTimeMs -> 3361, numDeletionVectorsUpdated -> 0, scanTimeMs -> 1761, numAddedFiles -> 1, numUpdatedRows -> 1, numAddedBytes -> 1260, rewriteTimeMs -> 1585)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>1</td><td>2025-06-14T13:32:01Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(3289096376091242)</td><td>0614-132416-8nqtb2y</td><td>0</td><td>WriteSerializable</td><td>true</td><td>Map(numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1257)</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr><tr><td>0</td><td>2025-06-14T13:30:05Z</td><td>5787056554928740</td><td>thiagolsmail@gmail.com</td><td>CREATE TABLE</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {}, statsOnLoad -> false)</td><td>null</td><td>List(3289096376091242)</td><td>0614-132416-8nqtb2y</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/15.4.x-scala2.12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         7,
         "2025-06-14T14:40:31Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "OPTIMIZE",
         {
          "auto": "false",
          "batchId": "0",
          "clusterBy": "[]",
          "predicate": "[]",
          "zOrderBy": "[\"id\"]"
         },
         null,
         [
          "3289096376091294"
         ],
         "0614-132416-8nqtb2y",
         6,
         "SnapshotIsolation",
         false,
         {
          "maxFileSize": "1551",
          "minFileSize": "1551",
          "numAddedBytes": "1551",
          "numAddedFiles": "1",
          "numDeletionVectorsRemoved": "0",
          "numRemovedBytes": "3783",
          "numRemovedFiles": "3",
          "p25FileSize": "1551",
          "p50FileSize": "1551",
          "p75FileSize": "1551"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         6,
         "2025-06-14T14:37:28Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3289096376091294"
         ],
         "0614-132416-8nqtb2y",
         5,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1260",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         5,
         "2025-06-14T14:37:17Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3289096376091294"
         ],
         "0614-132416-8nqtb2y",
         4,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1263",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         4,
         "2025-06-14T14:03:38Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "RESTORE",
         {
          "timestamp": null,
          "version": "2"
         },
         null,
         [
          "3289096376091265"
         ],
         "0614-132416-8nqtb2y",
         3,
         "Serializable",
         false,
         {
          "numOfFilesAfterRestore": "1",
          "numRemovedFiles": "0",
          "numRestoredFiles": "1",
          "removedFilesSize": "0",
          "restoredFilesSize": "1260",
          "tableSizeAfterRestore": "1260"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         3,
         "2025-06-14T14:02:24Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "DELETE",
         {
          "predicate": "[\"true\"]"
         },
         null,
         [
          "3289096376091265"
         ],
         "0614-132416-8nqtb2y",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "66",
          "numAddedBytes": "0",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "0",
          "numCopiedRows": "0",
          "numDeletedRows": "5",
          "numDeletionVectorsAdded": "0",
          "numDeletionVectorsRemoved": "0",
          "numDeletionVectorsUpdated": "0",
          "numRemovedBytes": "1260",
          "numRemovedFiles": "1",
          "rewriteTimeMs": "0",
          "scanTimeMs": "65"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         2,
         "2025-06-14T13:37:06Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "UPDATE",
         {
          "predicate": "[\"(nome#1479 = Carlos)\"]"
         },
         null,
         [
          "3289096376091242"
         ],
         "0614-132416-8nqtb2y",
         1,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "3361",
          "numAddedBytes": "1260",
          "numAddedChangeFiles": "0",
          "numAddedFiles": "1",
          "numCopiedRows": "4",
          "numDeletionVectorsAdded": "0",
          "numDeletionVectorsRemoved": "0",
          "numDeletionVectorsUpdated": "0",
          "numRemovedBytes": "1257",
          "numRemovedFiles": "1",
          "numUpdatedRows": "1",
          "rewriteTimeMs": "1585",
          "scanTimeMs": "1761"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         1,
         "2025-06-14T13:32:01Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "3289096376091242"
         ],
         "0614-132416-8nqtb2y",
         0,
         "WriteSerializable",
         true,
         {
          "numFiles": "1",
          "numOutputBytes": "1257",
          "numOutputRows": "5"
         },
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ],
        [
         0,
         "2025-06-14T13:30:05Z",
         "5787056554928740",
         "thiagolsmail@gmail.com",
         "CREATE TABLE",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{}",
          "statsOnLoad": "false"
         },
         null,
         [
          "3289096376091242"
         ],
         "0614-132416-8nqtb2y",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/15.4.x-scala2.12"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 6
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"jobId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobName\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobRunId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"runId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"jobOwnerId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"triggerType\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"notebookId\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"type\":\"map\",\"keyType\":\"string\",\"valueType\":\"string\",\"valueContainsNull\":true}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1418c4e7-6675-40f4-9737-a23af7d4d11e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/</td><td>_delta_log/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet</td><td>part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet</td><td>1263</td><td>1749911837000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet</td><td>part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet</td><td>1257</td><td>1749907920000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet</td><td>part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet</td><td>1260</td><td>1749908225000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet</td><td>part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet</td><td>1551</td><td>1749912030000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet</td><td>part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet</td><td>1260</td><td>1749911847000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/",
         "_delta_log/",
         0,
         0
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet",
         "part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet",
         1263,
         1749911837000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet",
         "part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet",
         1257,
         1749907920000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet",
         "part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet",
         1260,
         1749908225000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet",
         "part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet",
         1551,
         1749912030000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet",
         "part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet",
         1260,
         1749911847000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls \"dbfs:/user/hive/warehouse/client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "305bbaf4-a66f-4bb2-8a42-0ec3ca96d16e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000000.crc</td><td>00000000000000000000.crc</td><td>2047</td><td>1749907821000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000000.json</td><td>00000000000000000000.json</td><td>1070</td><td>1749907805000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000001.crc</td><td>00000000000000000001.crc</td><td>2601</td><td>1749907926000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000001.json</td><td>00000000000000000001.json</td><td>1154</td><td>1749907921000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000002.crc</td><td>00000000000000000002.crc</td><td>2601</td><td>1749908230000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000002.json</td><td>00000000000000000002.json</td><td>1833</td><td>1749908226000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000003.crc</td><td>00000000000000000003.crc</td><td>2047</td><td>1749909748000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000003.json</td><td>00000000000000000003.json</td><td>1250</td><td>1749909744000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000004.checkpoint.parquet</td><td>00000000000000000004.checkpoint.parquet</td><td>21333</td><td>1749909822000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000004.crc</td><td>00000000000000000004.crc</td><td>2018</td><td>1749909821000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000004.json</td><td>00000000000000000004.json</td><td>1665</td><td>1749909818000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000005.crc</td><td>00000000000000000005.crc</td><td>3151</td><td>1749911842000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000005.json</td><td>00000000000000000005.json</td><td>1155</td><td>1749911837000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000006.crc</td><td>00000000000000000006.crc</td><td>3703</td><td>1749911850000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000006.json</td><td>00000000000000000006.json</td><td>1157</td><td>1749911848000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000007.crc</td><td>00000000000000000007.crc</td><td>2717</td><td>1749912035000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000007.json</td><td>00000000000000000007.json</td><td>2636</td><td>1749912031000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/_commits/</td><td>_commits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/_last_checkpoint</td><td>_last_checkpoint</td><td>5895</td><td>1749909822000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000000.crc",
         "00000000000000000000.crc",
         2047,
         1749907821000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000000.json",
         "00000000000000000000.json",
         1070,
         1749907805000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000001.crc",
         "00000000000000000001.crc",
         2601,
         1749907926000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000001.json",
         "00000000000000000001.json",
         1154,
         1749907921000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000002.crc",
         "00000000000000000002.crc",
         2601,
         1749908230000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000002.json",
         "00000000000000000002.json",
         1833,
         1749908226000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000003.crc",
         "00000000000000000003.crc",
         2047,
         1749909748000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000003.json",
         "00000000000000000003.json",
         1250,
         1749909744000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000004.checkpoint.parquet",
         "00000000000000000004.checkpoint.parquet",
         21333,
         1749909822000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000004.crc",
         "00000000000000000004.crc",
         2018,
         1749909821000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000004.json",
         "00000000000000000004.json",
         1665,
         1749909818000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000005.crc",
         "00000000000000000005.crc",
         3151,
         1749911842000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000005.json",
         "00000000000000000005.json",
         1155,
         1749911837000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000006.crc",
         "00000000000000000006.crc",
         3703,
         1749911850000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000006.json",
         "00000000000000000006.json",
         1157,
         1749911848000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000007.crc",
         "00000000000000000007.crc",
         2717,
         1749912035000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000007.json",
         "00000000000000000007.json",
         2636,
         1749912031000
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/_commits/",
         "_commits/",
         0,
         0
        ],
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/_last_checkpoint",
         "_last_checkpoint",
         5895,
         1749909822000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls \"dbfs:/user/hive/warehouse/client/_delta_log/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27752681-7663-4f25-9005-1788a9a4f2d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">{&quot;commitInfo&quot;:{&quot;timestamp&quot;:1749912030040,&quot;userId&quot;:&quot;5787056554928740&quot;,&quot;userName&quot;:&quot;thiagolsmail@gmail.com&quot;,&quot;operation&quot;:&quot;OPTIMIZE&quot;,&quot;operationParameters&quot;:{&quot;predicate&quot;:&quot;[]&quot;,&quot;auto&quot;:false,&quot;clusterBy&quot;:&quot;[]&quot;,&quot;zOrderBy&quot;:&quot;[\\&quot;id\\&quot;]&quot;,&quot;batchId&quot;:&quot;0&quot;},&quot;notebook&quot;:{&quot;notebookId&quot;:&quot;3289096376091294&quot;},&quot;clusterId&quot;:&quot;0614-132416-8nqtb2y&quot;,&quot;readVersion&quot;:6,&quot;isolationLevel&quot;:&quot;SnapshotIsolation&quot;,&quot;isBlindAppend&quot;:false,&quot;operationMetrics&quot;:{&quot;numRemovedFiles&quot;:&quot;3&quot;,&quot;numRemovedBytes&quot;:&quot;3783&quot;,&quot;p25FileSize&quot;:&quot;1551&quot;,&quot;numDeletionVectorsRemoved&quot;:&quot;0&quot;,&quot;minFileSize&quot;:&quot;1551&quot;,&quot;numAddedFiles&quot;:&quot;1&quot;,&quot;maxFileSize&quot;:&quot;1551&quot;,&quot;p75FileSize&quot;:&quot;1551&quot;,&quot;p50FileSize&quot;:&quot;1551&quot;,&quot;numAddedBytes&quot;:&quot;1551&quot;},&quot;tags&quot;:{&quot;delta.rowTracking.preserved&quot;:&quot;false&quot;},&quot;engineInfo&quot;:&quot;Databricks-Runtime/15.4.x-scala2.12&quot;,&quot;txnId&quot;:&quot;23fe5854-86c1-4e8a-8be0-c05c36a9d079&quot;}}\n",
       "{&quot;remove&quot;:{&quot;path&quot;:&quot;part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet&quot;,&quot;deletionTimestamp&quot;:1749912028619,&quot;dataChange&quot;:false,&quot;extendedFileMetadata&quot;:true,&quot;partitionValues&quot;:{},&quot;size&quot;:1263,&quot;tags&quot;:{&quot;INSERTION_TIME&quot;:&quot;1749911837000000&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749911837000000&quot;,&quot;MAX_INSERTION_TIME&quot;:&quot;1749911837000000&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;},&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:5}&quot;}}\n",
       "{&quot;remove&quot;:{&quot;path&quot;:&quot;part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet&quot;,&quot;deletionTimestamp&quot;:1749912028619,&quot;dataChange&quot;:false,&quot;extendedFileMetadata&quot;:true,&quot;partitionValues&quot;:{},&quot;size&quot;:1260,&quot;tags&quot;:{&quot;MAX_INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;},&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:5}&quot;}}\n",
       "{&quot;remove&quot;:{&quot;path&quot;:&quot;part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet&quot;,&quot;deletionTimestamp&quot;:1749912028619,&quot;dataChange&quot;:false,&quot;extendedFileMetadata&quot;:true,&quot;partitionValues&quot;:{},&quot;size&quot;:1260,&quot;tags&quot;:{&quot;INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;MAX_INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;},&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:5}&quot;}}\n",
       "{&quot;add&quot;:{&quot;path&quot;:&quot;part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet&quot;,&quot;partitionValues&quot;:{},&quot;size&quot;:1551,&quot;modificationTime&quot;:1749912030000,&quot;dataChange&quot;:false,&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:15,\\&quot;minValues\\&quot;:{\\&quot;id\\&quot;:1,\\&quot;nome\\&quot;:\\&quot;Adam\\&quot;,\\&quot;email\\&quot;:\\&quot;adam@example.com\\&quot;},\\&quot;maxValues\\&quot;:{\\&quot;id\\&quot;:15,\\&quot;nome\\&quot;:\\&quot;Vinicius\\&quot;,\\&quot;email\\&quot;:\\&quot;vini@example.org\\&quot;},\\&quot;nullCount\\&quot;:{\\&quot;id\\&quot;:0,\\&quot;nome\\&quot;:0,\\&quot;email\\&quot;:0}}&quot;,&quot;tags&quot;:{&quot;MAX_INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;ZCUBE_ZORDER_CURVE&quot;:&quot;hilbert&quot;,&quot;ZCUBE_ZORDER_BY&quot;:&quot;[\\&quot;id\\&quot;]&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;ZCUBE_ID&quot;:&quot;5be565dc-ad50-4301-801f-a77789c25e92&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;}}}\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">{&quot;commitInfo&quot;:{&quot;timestamp&quot;:1749912030040,&quot;userId&quot;:&quot;5787056554928740&quot;,&quot;userName&quot;:&quot;thiagolsmail@gmail.com&quot;,&quot;operation&quot;:&quot;OPTIMIZE&quot;,&quot;operationParameters&quot;:{&quot;predicate&quot;:&quot;[]&quot;,&quot;auto&quot;:false,&quot;clusterBy&quot;:&quot;[]&quot;,&quot;zOrderBy&quot;:&quot;[\\&quot;id\\&quot;]&quot;,&quot;batchId&quot;:&quot;0&quot;},&quot;notebook&quot;:{&quot;notebookId&quot;:&quot;3289096376091294&quot;},&quot;clusterId&quot;:&quot;0614-132416-8nqtb2y&quot;,&quot;readVersion&quot;:6,&quot;isolationLevel&quot;:&quot;SnapshotIsolation&quot;,&quot;isBlindAppend&quot;:false,&quot;operationMetrics&quot;:{&quot;numRemovedFiles&quot;:&quot;3&quot;,&quot;numRemovedBytes&quot;:&quot;3783&quot;,&quot;p25FileSize&quot;:&quot;1551&quot;,&quot;numDeletionVectorsRemoved&quot;:&quot;0&quot;,&quot;minFileSize&quot;:&quot;1551&quot;,&quot;numAddedFiles&quot;:&quot;1&quot;,&quot;maxFileSize&quot;:&quot;1551&quot;,&quot;p75FileSize&quot;:&quot;1551&quot;,&quot;p50FileSize&quot;:&quot;1551&quot;,&quot;numAddedBytes&quot;:&quot;1551&quot;},&quot;tags&quot;:{&quot;delta.rowTracking.preserved&quot;:&quot;false&quot;},&quot;engineInfo&quot;:&quot;Databricks-Runtime/15.4.x-scala2.12&quot;,&quot;txnId&quot;:&quot;23fe5854-86c1-4e8a-8be0-c05c36a9d079&quot;}}\n{&quot;remove&quot;:{&quot;path&quot;:&quot;part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet&quot;,&quot;deletionTimestamp&quot;:1749912028619,&quot;dataChange&quot;:false,&quot;extendedFileMetadata&quot;:true,&quot;partitionValues&quot;:{},&quot;size&quot;:1263,&quot;tags&quot;:{&quot;INSERTION_TIME&quot;:&quot;1749911837000000&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749911837000000&quot;,&quot;MAX_INSERTION_TIME&quot;:&quot;1749911837000000&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;},&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:5}&quot;}}\n{&quot;remove&quot;:{&quot;path&quot;:&quot;part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet&quot;,&quot;deletionTimestamp&quot;:1749912028619,&quot;dataChange&quot;:false,&quot;extendedFileMetadata&quot;:true,&quot;partitionValues&quot;:{},&quot;size&quot;:1260,&quot;tags&quot;:{&quot;MAX_INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;},&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:5}&quot;}}\n{&quot;remove&quot;:{&quot;path&quot;:&quot;part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet&quot;,&quot;deletionTimestamp&quot;:1749912028619,&quot;dataChange&quot;:false,&quot;extendedFileMetadata&quot;:true,&quot;partitionValues&quot;:{},&quot;size&quot;:1260,&quot;tags&quot;:{&quot;INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;MAX_INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;},&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:5}&quot;}}\n{&quot;add&quot;:{&quot;path&quot;:&quot;part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet&quot;,&quot;partitionValues&quot;:{},&quot;size&quot;:1551,&quot;modificationTime&quot;:1749912030000,&quot;dataChange&quot;:false,&quot;stats&quot;:&quot;{\\&quot;numRecords\\&quot;:15,\\&quot;minValues\\&quot;:{\\&quot;id\\&quot;:1,\\&quot;nome\\&quot;:\\&quot;Adam\\&quot;,\\&quot;email\\&quot;:\\&quot;adam@example.com\\&quot;},\\&quot;maxValues\\&quot;:{\\&quot;id\\&quot;:15,\\&quot;nome\\&quot;:\\&quot;Vinicius\\&quot;,\\&quot;email\\&quot;:\\&quot;vini@example.org\\&quot;},\\&quot;nullCount\\&quot;:{\\&quot;id\\&quot;:0,\\&quot;nome\\&quot;:0,\\&quot;email\\&quot;:0}}&quot;,&quot;tags&quot;:{&quot;MAX_INSERTION_TIME&quot;:&quot;1749911847000000&quot;,&quot;INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;ZCUBE_ZORDER_CURVE&quot;:&quot;hilbert&quot;,&quot;ZCUBE_ZORDER_BY&quot;:&quot;[\\&quot;id\\&quot;]&quot;,&quot;MIN_INSERTION_TIME&quot;:&quot;1749907920000000&quot;,&quot;ZCUBE_ID&quot;:&quot;5be565dc-ad50-4301-801f-a77789c25e92&quot;,&quot;OPTIMIZE_TARGET_SIZE&quot;:&quot;268435456&quot;}}}\n\n</div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs head \"dbfs:/user/hive/warehouse/client/_delta_log/00000000000000000007.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00016751-2640-43b7-b145-0c6ad843685a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 7
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "VACUUM client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f13849e-a152-4a27-89ac-f0821bbb68f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/</td><td>_delta_log/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet</td><td>part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet</td><td>1263</td><td>1749911837000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet</td><td>part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet</td><td>1257</td><td>1749907920000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet</td><td>part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet</td><td>1260</td><td>1749908225000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet</td><td>part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet</td><td>1551</td><td>1749912030000</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet</td><td>part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet</td><td>1260</td><td>1749911847000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/",
         "_delta_log/",
         0,
         0
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet",
         "part-00000-0bcddde0-e415-48c6-9e98-298d2254429d-c000.snappy.parquet",
         1263,
         1749911837000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet",
         "part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet",
         1257,
         1749907920000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet",
         "part-00000-a9a3dacd-a9c9-4015-84d5-519f3d20530a-c000.snappy.parquet",
         1260,
         1749908225000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet",
         "part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet",
         1551,
         1749912030000
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet",
         "part-00000-f1795f6f-3606-4d3e-b4fe-39effb41b986-c000.snappy.parquet",
         1260,
         1749911847000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls \"dbfs:/user/hive/warehouse/client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e4de3f9-b666-4b22-83ae-6fedd59f1c1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\n",
       "writers that are currently writing to this table, there is a risk that you may corrupt the\n",
       "state of your Delta table.\n",
       "If you are certain that there are no operations being performed on this table, such as\n",
       "insert/upsert/delete/optimize, then you may turn off this check by setting:\n",
       "spark.databricks.delta.retentionDurationCheck.enabled = false\n",
       "If you are not sure, please use a value not less than \"168 hours\".\n",
       "       \n",
       "\tat scala.Predef$.require(Predef.scala:281)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.checkRetentionPeriodSafety(VacuumCommand.scala:113)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.getValidFilesFromSnapshot(VacuumCommand.scala:133)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.$anonfun$gc$2(VacuumCommand.scala:317)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:293)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:280)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.withOperationTypeTag(VacuumCommand.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:386)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:384)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.recordFrameProfile(VacuumCommand.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)\n",
       "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:87)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:173)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)\n",
       "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)\n",
       "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:132)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)\n",
       "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.recordOperation(VacuumCommand.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)\n",
       "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.recordDeltaOperation(VacuumCommand.scala:70)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.$anonfun$gc$1(VacuumCommand.scala:239)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$1(CheckPermissions.scala:2291)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:159)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2291)\n",
       "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.gc(VacuumCommand.scala:239)\n",
       "\tat com.databricks.sql.transaction.directory.VacuumTableCommand.$anonfun$run$1(VacuumTableCommand.scala:293)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2311)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)\n",
       "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)\n",
       "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:159)\n",
       "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2311)\n",
       "\tat com.databricks.sql.transaction.directory.VacuumTableCommand.run(VacuumTableCommand.scala:145)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:189)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)\n",
       "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$5(QueryExecution.scala:434)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$4(QueryExecution.scala:434)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:434)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:475)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:826)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:334)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:207)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:763)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:430)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1237)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:426)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:364)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:423)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:511)\n",
       "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:511)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:487)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:397)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:397)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:323)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:320)\n",
       "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:343)\n",
       "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1217)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1217)\n",
       "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)\n",
       "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:989)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:973)\n",
       "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1012)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:324)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:424)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:446)\n",
       "\tat scala.collection.immutable.List.map(List.scala:293)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:441)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:498)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)\n",
       "\tat java.lang.Thread.run(Thread.java:750)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       "
       },
       "removedWidgets": [],
       "sqlProps": null,
       "stackFrames": [
        "com.databricks.backend.common.rpc.SparkDriverExceptions$SQLExecutionException: java.lang.IllegalArgumentException: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have",
        "writers that are currently writing to this table, there is a risk that you may corrupt the",
        "state of your Delta table.",
        "If you are certain that there are no operations being performed on this table, such as",
        "insert/upsert/delete/optimize, then you may turn off this check by setting:",
        "spark.databricks.delta.retentionDurationCheck.enabled = false",
        "If you are not sure, please use a value not less than \"168 hours\".",
        "       ",
        "\tat scala.Predef$.require(Predef.scala:281)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.checkRetentionPeriodSafety(VacuumCommand.scala:113)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.getValidFilesFromSnapshot(VacuumCommand.scala:133)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.$anonfun$gc$2(VacuumCommand.scala:317)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag(DeltaLogging.scala:293)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.withOperationTypeTag$(DeltaLogging.scala:280)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.withOperationTypeTag(VacuumCommand.scala:70)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$2(DeltaLogging.scala:177)",
        "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:386)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:384)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.recordFrameProfile(VacuumCommand.scala:70)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:176)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)",
        "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)",
        "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.spark.util.PublicDBLogging.withAttributionTags(DatabricksSparkUsageLogger.scala:30)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)",
        "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)",
        "\tat com.databricks.spark.util.PublicDBLogging.recordOperationWithResultTags(DatabricksSparkUsageLogger.scala:30)",
        "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)",
        "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)",
        "\tat com.databricks.spark.util.PublicDBLogging.recordOperation(DatabricksSparkUsageLogger.scala:30)",
        "\tat com.databricks.spark.util.PublicDBLogging.recordOperation0(DatabricksSparkUsageLogger.scala:87)",
        "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:173)",
        "\tat com.databricks.spark.util.UsageLogger.recordOperation(UsageLogger.scala:78)",
        "\tat com.databricks.spark.util.UsageLogger.recordOperation$(UsageLogger.scala:65)",
        "\tat com.databricks.spark.util.DatabricksSparkUsageLogger.recordOperation(DatabricksSparkUsageLogger.scala:132)",
        "\tat com.databricks.spark.util.UsageLogging.recordOperation(UsageLogger.scala:537)",
        "\tat com.databricks.spark.util.UsageLogging.recordOperation$(UsageLogger.scala:516)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.recordOperation(VacuumCommand.scala:70)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:175)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:165)",
        "\tat com.databricks.sql.transaction.tahoe.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:154)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.recordDeltaOperation(VacuumCommand.scala:70)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.$anonfun$gc$1(VacuumCommand.scala:239)",
        "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$1(CheckPermissions.scala:2291)",
        "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)",
        "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)",
        "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:159)",
        "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2291)",
        "\tat com.databricks.sql.transaction.tahoe.commands.VacuumCommand$.gc(VacuumCommand.scala:239)",
        "\tat com.databricks.sql.transaction.directory.VacuumTableCommand.$anonfun$run$1(VacuumTableCommand.scala:293)",
        "\tat com.databricks.sql.acl.CheckPermissions$.$anonfun$trusted$2(CheckPermissions.scala:2311)",
        "\tat com.databricks.sql.util.ThreadLocalTagger.withTag(QueryTagger.scala:62)",
        "\tat com.databricks.sql.util.ThreadLocalTagger.withTag$(QueryTagger.scala:59)",
        "\tat com.databricks.sql.util.QueryTagger$.withTag(QueryTagger.scala:159)",
        "\tat com.databricks.sql.acl.CheckPermissions$.trusted(CheckPermissions.scala:2311)",
        "\tat com.databricks.sql.transaction.directory.VacuumTableCommand.run(VacuumTableCommand.scala:145)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$2(commands.scala:84)",
        "\tat org.apache.spark.sql.execution.SparkPlan.runCommandInAetherOrSpark(SparkPlan.scala:189)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.$anonfun$sideEffectResult$1(commands.scala:84)",
        "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:81)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:80)",
        "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:94)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$5(QueryExecution.scala:434)",
        "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$4(QueryExecution.scala:434)",
        "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:194)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$3(QueryExecution.scala:434)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:475)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:826)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:334)",
        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:207)",
        "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:763)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$2(QueryExecution.scala:430)",
        "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1237)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.$anonfun$applyOrElse$1(QueryExecution.scala:426)",
        "\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$withMVTagsIfNecessary(QueryExecution.scala:364)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:423)",
        "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$$nestedInanonfun$eagerlyExecuteCommands$1$1.applyOrElse(QueryExecution.scala:397)",
        "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:511)",
        "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:85)",
        "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:511)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:40)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:379)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:375)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:40)",
        "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:487)",
        "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:397)",
        "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:436)",
        "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:397)",
        "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:323)",
        "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:320)",
        "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:343)",
        "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:131)",
        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)",
        "\tat org.apache.spark.sql.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1217)",
        "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)",
        "\tat org.apache.spark.sql.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1217)",
        "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:122)",
        "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$4(SparkSession.scala:989)",
        "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)",
        "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:973)",
        "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:1012)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal$DbClassicStrategy.executeSQLQuery(DriverLocal.scala:324)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSQLSubCommand(DriverLocal.scala:424)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$executeSql$1(DriverLocal.scala:446)",
        "\tat scala.collection.immutable.List.map(List.scala:293)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:441)",
        "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)",
        "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)",
        "\tat scala.util.Try$.apply(Try.scala:213)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)",
        "\tat java.lang.Thread.run(Thread.java:750)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:498)",
        "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)",
        "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)",
        "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)",
        "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)",
        "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)",
        "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)",
        "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)",
        "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)",
        "\tat scala.util.Try$.apply(Try.scala:213)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)",
        "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)",
        "\tat java.lang.Thread.run(Thread.java:750)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "VACUUM client RETAIN 0 HOURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6b60079-8b16-4d62-a6ee-ce9bf7cc58fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>key</th><th>value</th></tr></thead><tbody><tr><td>spark.databricks.delta.retentionDurationCheck.enabled</td><td>False</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "spark.databricks.delta.retentionDurationCheck.enabled",
         "False"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 8
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "key",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "value",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SET spark.databricks.delta.retentionDurationCheck.enabled = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d332afc-a9af-4c54-8f09-3356881be936",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 9
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "VACUUM client RETAIN 0 HOURS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37756e77-e3ae-4984-b829-27544d577dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/user/hive/warehouse/client/_delta_log/</td><td>_delta_log/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/user/hive/warehouse/client/part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet</td><td>part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet</td><td>1551</td><td>1749912030000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/user/hive/warehouse/client/_delta_log/",
         "_delta_log/",
         0,
         0
        ],
        [
         "dbfs:/user/hive/warehouse/client/part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet",
         "part-00000-e9d7e3c8-72ec-4f68-b372-ebb2d95c8ef7-c000.snappy.parquet",
         1551,
         1749912030000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls \"dbfs:/user/hive/warehouse/client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31119c4d-24ce-43a9-b102-39f99da2a122",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet. [DELTA_FILE_NOT_FOUND_DETAILED] File dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions SQLSTATE: KD001\n",
       "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logMissingFileNameAndThrow(FileScanRDD.scala:796)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:692)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:982)\n",
       "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1413)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1401)\n",
       "\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3171)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n",
       "\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n",
       "\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:552)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:546)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:563)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:401)\n",
       "\tat scala.Option.getOrElse(Option.scala:189)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:400)\n",
       "\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:319)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:572)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n",
       "\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:569)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3844)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3835)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4807)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1237)\n",
       "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4805)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:475)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:826)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:334)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:207)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:763)\n",
       "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4805)\n",
       "\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3834)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:325)\n",
       "\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:464)\n",
       "\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)\n",
       "\tat java.lang.Thread.run(Thread.java:750)\n",
       "Caused by: java.io.FileNotFoundException: /user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:126)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:91)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$6(DbfsHadoop3.scala:79)\n",
       "\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:79)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4913)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.openFileWithOptions(LokiFileSystem.scala:335)\n",
       "\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4913)\n",
       "\tat com.databricks.spark.util.DatabricksRangeFetcher.fetchRange(DatabricksRangeFetcher.scala:73)\n",
       "\tat com.databricks.io.RangeFetcher$.fetch(RangeFetcher.scala:104)\n",
       "\tat com.databricks.io.RangeFetcher.fetch(RangeFetcher.scala)\n",
       "\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:104)\n",
       "\tat com.databricks.sql.io.Futures.run(Futures.java:74)\n",
       "\tat com.databricks.sql.io.PendingFutures.submit(PendingFutures.java:63)\n",
       "\tat com.databricks.sql.io.Storage$ReadFile.submit(Storage.java:189)\n",
       "\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.fetchRange(HDFSStorage.java:101)\n",
       "\tat com.databricks.sql.io.Storage.fetchRange(Storage.java:210)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.readTail(CachingParquetFileReader.java:338)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:358)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$2(CachingParquetFooterReader.java:220)\n",
       "\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:18)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:214)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:134)\n",
       "\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:387)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:162)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:415)\n",
       "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:258)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:655)\n",
       "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n",
       "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:982)\n",
       "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
       "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
       "\tat scala.util.Success.map(Try.scala:213)\n",
       "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
       "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
       "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n",
       "\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n",
       "\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
       "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
       "\t... 1 more\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": null,
       "metadata": {
        "errorSummary": "[FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet. [DELTA_FILE_NOT_FOUND_DETAILED] File dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions SQLSTATE: KD001"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "FAILED_READ_FILE.DBR_FILE_NOT_EXIST",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "pysparkSummary": null,
        "sqlState": "KD001",
        "stackTrace": null,
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "org.apache.spark.SparkException: [FAILED_READ_FILE.DBR_FILE_NOT_EXIST] Error while reading file dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet. [DELTA_FILE_NOT_FOUND_DETAILED] File dbfs:/user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet referenced in the transaction log cannot be found. This occurs when data has been manually deleted from the file system rather than using the table `DELETE` statement. For more information, see https://docs.databricks.com/delta/delta-intro.html#frequently-asked-questions SQLSTATE: KD001\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.fileNotExistErrorDBR(QueryExecutionErrors.scala:1069)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logMissingFileNameAndThrow(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:692)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:982)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1413)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1401)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:3171)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:355)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:299)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$collect$1(Collector.scala:384)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:381)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:122)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:552)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:546)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:563)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:401)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:400)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:319)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:572)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:569)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3844)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectResult$1(Dataset.scala:3835)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4807)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1237)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4805)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$10(SQLExecution.scala:475)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:826)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:334)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1210)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:207)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:763)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4805)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3834)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:325)\n\tat com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:101)\n\tat com.databricks.backend.daemon.driver.DriverLocal.executeSql(DriverLocal.scala:464)\n\tat com.databricks.backend.daemon.driver.JupyterDriverLocal.repl(JupyterDriverLocal.scala:1054)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.io.FileNotFoundException: /user/hive/warehouse/client/part-00000-7068a226-f8c5-43b6-86e1-4d8506f42476-c000.snappy.parquet\n\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:126)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:80)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.open(DatabricksFileSystemV1.scala:89)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.open(DatabricksFileSystem.scala:91)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.$anonfun$openFileWithOptions$6(DbfsHadoop3.scala:79)\n\tat org.apache.hadoop.util.LambdaUtils.eval(LambdaUtils.java:52)\n\tat com.databricks.backend.daemon.data.client.DbfsHadoop3.openFileWithOptions(DbfsHadoop3.scala:79)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4913)\n\tat com.databricks.common.filesystem.LokiFileSystem.openFileWithOptions(LokiFileSystem.scala:335)\n\tat org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder.build(FileSystem.java:4913)\n\tat com.databricks.spark.util.DatabricksRangeFetcher.fetchRange(DatabricksRangeFetcher.scala:73)\n\tat com.databricks.io.RangeFetcher$.fetch(RangeFetcher.scala:104)\n\tat com.databricks.io.RangeFetcher.fetch(RangeFetcher.scala)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.lambda$fetchRange$0(HDFSStorage.java:104)\n\tat com.databricks.sql.io.Futures.run(Futures.java:74)\n\tat com.databricks.sql.io.PendingFutures.submit(PendingFutures.java:63)\n\tat com.databricks.sql.io.Storage$ReadFile.submit(Storage.java:189)\n\tat com.databricks.sql.io.HDFSStorage$ReadFileImpl.fetchRange(HDFSStorage.java:101)\n\tat com.databricks.sql.io.Storage.fetchRange(Storage.java:210)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.readTail(CachingParquetFileReader.java:338)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader$FooterByteReader.read(CachingParquetFileReader.java:358)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.lambda$readFooterFromStorage$2(CachingParquetFooterReader.java:220)\n\tat org.apache.spark.util.JavaFrameProfiler.record(JavaFrameProfiler.java:18)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooterFromStorage(CachingParquetFooterReader.java:214)\n\tat com.databricks.sql.io.parquet.CachingParquetFooterReader.readFooter(CachingParquetFooterReader.java:134)\n\tat com.databricks.sql.io.parquet.CachingParquetFileReader.readFooter(CachingParquetFileReader.java:387)\n\tat org.apache.spark.sql.execution.datasources.parquet.SpecificParquetRecordReaderBase.prepare(SpecificParquetRecordReaderBase.java:162)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:415)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anon$1.apply(ParquetFileFormat.scala:258)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:655)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$prepareNextFile$1(FileScanRDD.scala:982)\n\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n\tat scala.util.Success.map(Try.scala:213)\n\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:113)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:112)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:89)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:154)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:157)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM client@v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7828b139-2641-43fa-a684-91fdeebc10ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": null
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DROP TABLE client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc54be74-db15-4376-b16d-d94e1687c9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {
        "isDbfsCommandResult": false
       },
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:126)\n",
       "\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n",
       "\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:165)\n",
       "\tat com.databricks.common.filesystem.LokiFileSystem.listStatus(LokiFileSystem.scala:306)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:300)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:267)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:154)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:267)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:149)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:266)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:239)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n",
       "\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n",
       "\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n",
       "\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:76)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:140)\n",
       "\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:239)\n",
       "\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:1)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:43)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:45)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:47)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw.&lt;init&gt;(command-3289096376091313:49)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw.&lt;init&gt;(command-3289096376091313:51)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read.&lt;init&gt;(command-3289096376091313:53)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$.&lt;init&gt;(command-3289096376091313:57)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$.&lt;clinit&gt;(command-3289096376091313)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$eval$.$print(&lt;notebook&gt;:6)\n",
       "\tat $line1cf7565688194ec597dfcb3245c04a0b35.$eval.$print(&lt;notebook&gt;)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
       "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
       "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
       "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
       "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n",
       "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n",
       "\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n",
       "\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n",
       "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n",
       "\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n",
       "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n",
       "\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:201)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$3(ScalaDriverLocal.scala:295)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.threadSafeTrapExit(DriverLocal.scala:1632)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1590)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1514)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.executeCommand$1(ScalaDriverLocal.scala:295)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$2(ScalaDriverLocal.scala:265)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat scala.Console$.withErr(Console.scala:196)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:262)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat scala.Console$.withOut(Console.scala:167)\n",
       "\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:262)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)\n",
       "\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)\n",
       "\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n",
       "\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)\n",
       "\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)\n",
       "\tat scala.util.Try$.apply(Try.scala:213)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n",
       "\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)\n",
       "\tat java.lang.Thread.run(Thread.java:750)</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "<div class=\"ansiout\">\tat com.databricks.backend.daemon.data.client.DbfsClient.send0(DbfsClient.scala:126)\n\tat com.databricks.backend.daemon.data.client.DbfsClient.sendIdempotent(DbfsClient.scala:74)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV1.listStatus(DatabricksFileSystemV1.scala:179)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.listStatus(DatabricksFileSystem.scala:165)\n\tat com.databricks.common.filesystem.LokiFileSystem.listStatus(LokiFileSystem.scala:306)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsWithLimit(DBUtilsCore.scala:300)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$4(DBUtilsCore.scala:267)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withFsSafetyCheck(DBUtilsCore.scala:154)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$lsImpl$3(DBUtilsCore.scala:267)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.checkPermission(DBUtilsCore.scala:149)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.lsImpl(DBUtilsCore.scala:266)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.$anonfun$ls$1(DBUtilsCore.scala:239)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperation$1(UsageLogging.scala:528)\n\tat com.databricks.logging.UsageLogging.executeThunkAndCaptureResultTags$1(UsageLogging.scala:633)\n\tat com.databricks.logging.UsageLogging.$anonfun$recordOperationWithResultTags$4(UsageLogging.scala:656)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionContext(DBUtilsCore.scala:76)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.withAttributionTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags(UsageLogging.scala:628)\n\tat com.databricks.logging.UsageLogging.recordOperationWithResultTags$(UsageLogging.scala:537)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperationWithResultTags(DBUtilsCore.scala:76)\n\tat com.databricks.logging.UsageLogging.recordOperation(UsageLogging.scala:529)\n\tat com.databricks.logging.UsageLogging.recordOperation$(UsageLogging.scala:495)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordOperation(DBUtilsCore.scala:76)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.recordDbutilsFsOp(DBUtilsCore.scala:140)\n\tat com.databricks.backend.daemon.dbutils.FSUtils.ls(DBUtilsCore.scala:239)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.ls(DbfsUtilsImpl.scala:67)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:1)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:43)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:45)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw$$iw.&lt;init&gt;(command-3289096376091313:47)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw$$iw.&lt;init&gt;(command-3289096376091313:49)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$$iw.&lt;init&gt;(command-3289096376091313:51)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read.&lt;init&gt;(command-3289096376091313:53)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$.&lt;init&gt;(command-3289096376091313:57)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$read$.&lt;clinit&gt;(command-3289096376091313)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$eval$.$print(&lt;notebook&gt;:6)\n\tat $line1cf7565688194ec597dfcb3245c04a0b35.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:747)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1020)\n\tat scala.tools.nsc.interpreter.IMain.$anonfun$interpret$1(IMain.scala:568)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext(ScalaClassLoader.scala:36)\n\tat scala.reflect.internal.util.ScalaClassLoader.asContext$(ScalaClassLoader.scala:116)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:41)\n\tat scala.tools.nsc.interpreter.IMain.loadAndRunReq$1(IMain.scala:567)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:594)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:564)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:201)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$3(ScalaDriverLocal.scala:295)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.threadSafeTrapExit(DriverLocal.scala:1632)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:1590)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:1514)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.executeCommand$1(ScalaDriverLocal.scala:295)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$2(ScalaDriverLocal.scala:265)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat scala.Console$.withErr(Console.scala:196)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.$anonfun$repl$1(ScalaDriverLocal.scala:262)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat scala.Console$.withOut(Console.scala:167)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:262)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$33(DriverLocal.scala:1195)\n\tat com.databricks.unity.EmptyHandle$.runWith(UCSHandle.scala:133)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$28(DriverLocal.scala:1186)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:48)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:276)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:272)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:46)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:43)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:97)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags(AttributionContextTracing.scala:95)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionTags$(AttributionContextTracing.scala:76)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:97)\n\tat com.databricks.backend.daemon.driver.DriverLocal.$anonfun$execute$1(DriverLocal.scala:1122)\n\tat com.databricks.backend.daemon.driver.DriverLocal$.$anonfun$maybeSynchronizeExecution$4(DriverLocal.scala:1542)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:799)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$2(DriverWrapper.scala:961)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.$anonfun$tryExecutingCommand$1(DriverWrapper.scala:950)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:982)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommandAndGetError(DriverWrapper.scala:746)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:814)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:615)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:541)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:335)\n\tat java.lang.Thread.run(Thread.java:750)</div>",
       "errorSummary": "FileNotFoundException: /user/hive/warehouse/client",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls \"dbfs:/user/hive/warehouse/client\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a2560c0-4ce8-4392-867b-dff81b456e34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3289096376091313,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Aplicando_Otimizacoes",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}